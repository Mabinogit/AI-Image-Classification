{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPY48DZm5icZoKw0r8FsGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mabinogit/AI-Image-Classification/blob/main/loss_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOOfunW7u9e-",
        "outputId": "061cc907-c5e1-4936-a65e-4fa27ffc0926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "DZIMhyc7yIHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothedCrossEntropy(nn.Module):\n",
        "  def __init__(self, epsilon=0.1, ignore_index=-100):\n",
        "        \"\"\"\n",
        "        Label smoothing loss function for Transformer models.\n",
        "\n",
        "        Args:\n",
        "        - epsilon (float): epsilon: This controls the amount of label smoothing applied. A higher epsilon means more smoothing. It defaults to 0.1..\n",
        "        - ignore_index (int): ignore_index: Specifies an index in the target labels that should be ignored during loss calculation (often used for padding tokens). It defaults to -100. .\n",
        "        \"\"\"\n",
        "        super(LabelSmoothedCrossEntropy, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "        def forward(self, logits, target):\n",
        "        \"\"\"\n",
        "        Computes the label-smoothed cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "        - logits (Tensor): Model output (batch_size, seq_len, vocab_size)\n",
        "        - target (Tensor): Ground-truth labels (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "        - loss (Tensor): Scalar loss value\n",
        "        \"\"\"\n",
        "        # Gets the number of classes in your problem (from the logits).#\n",
        "        num_classes = logits.size(-1)\n",
        "\n",
        "        # log_probs: Converts the logits into log probabilities using the log_softmax function.\n",
        "        log_probs = F.log_softmax(logits, dim=-1)  # Convert logits to log probabilities\n",
        "\n",
        "        # Create a one-hot representation of the target\n",
        "        with torch.no_grad():\n",
        "            # This line is crucial for initialization. It creates a tensor named true_dist that is filled with zeros and has the same shape as log_probs.\n",
        "            true_dist = torch.zeros_like(log_probs)\n",
        "            # This PyTorch function is used to create a one-hot encoding of the target labels.\n",
        "            true_dist.scatter_(-1, target.unsqueeze(-1), 1.0)  # One-hot encoding\n",
        "            # The final line in this block applies the label smoothing using\n",
        "            true_dist = (1 - self.epsilon) * true_dist + self.epsilon / num_classes  # Apply smoothing\n",
        "\n",
        "         # Compute negative log likelihood of each probability/likelihood in the tensor\n",
        "        loss = -true_dist * log_probs\n",
        "         # Sum values of tensor into one value\n",
        "        oss = loss.sum(dim=-1)  # Sum over vocab dimension\n",
        "\n",
        "# Example Scenario 1: Lower probabilities for incorrect words\n",
        "\n",
        " #     Word 1: 0.8 (high probability)\n",
        " #     Word 2: 0.1\n",
        " #     Word 3: 0.1\n",
        " #    NLL: -log(0.8) + -log(0.1) + -log(0.1) = 0.22 + 2.30 + 2.30 = 4.82\n",
        "\n",
        "\n",
        "\n",
        "        # Ignore padding index.--- It means you have defined a value to represent padding tokens (e.g., ignore_index = 0 or ignore_index = -100).\n",
        "        #  The code understands that there might be padding in your data, and it needs to take steps to handle it correctly during loss calculation.\n",
        "        if self.ignore_index is not None:\n",
        "            mask = target != self.ignore_index\n",
        "            loss = loss * mask  # Zero out loss for padding tokens\n",
        "            return loss.sum() / mask.sum()  # Normalize by non-padding tokens\n",
        "        else:\n",
        "            return loss.mean()  # Regular mean loss\n",
        "\n",
        "'''\n",
        "Batch size: 2\n",
        "Sequence length: 4\n",
        "Assume we have two sequences in our batch:\n",
        "Sequence 1: \"The cat sat on the mat\"\n",
        "Sequence 2: \"I love dogs\"\n",
        "\n",
        "target = [[2, 3, 4, 5, 6, 7, 0, 0],   # \"The cat sat on the mat\" + padding\n",
        "          [1, 8, 9, 0, 0, 0, 0, 0]]   # \"I love dogs\" + padding\n",
        "\n",
        "Assume ignore_index = 0 (padding token is 0)\n",
        "Let's say, after calculating the loss for each token and summing over the vocabulary dimension, we have the following reduced loss tensor\n",
        "\n",
        "loss_reduced = [[0.5, 0.2, 0.3, 0.1, 0.6, 0.4, 0.8, 0.9],\n",
        "                [0.7, 0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8]]\n",
        "\n",
        "\n",
        "Creating the Mask:\n",
        "  mask = target != ignore_index\n",
        "\n",
        "This will create the following mask:\n",
        "  mask = [[True, True, True, True, True, True, False, False],\n",
        "        [True, True, True, False, False, False, False, False]]\n",
        "\n",
        "\n",
        "Element-wise Multiplication:\n",
        "    loss_masked = [[0.5 * True, 0.2 * True, 0.3 * True, 0.1 * True, 0.6 * True, 0.4 * True, 0.8 * False, 0.9 * False],\n",
        "               [0.7 * True, 0.1 * True, 0.2 * True, 0.3 * False, 0.5 * False, 0.6 * False, 0.7 * False, 0.8 * False]]\n",
        "\n",
        "Since True is treated as 1 and False as 0 in numerical operations, this simplifies to:\n",
        "    loss_masked = [[0.5, 0.2, 0.3, 0.1, 0.6, 0.4, 0.0, 0.0],\n",
        "               [0.7, 0.1, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
        "\n",
        "# Normalize by non-padding tokens\n",
        "\n",
        "  loss_masked.sum() would calculate (0.5 + 0.2 + 0.3 + 0.1 + 0.6 + 0.4 + 0.0 + 0.0) + (0.7 + 0.1 + 0.2 + 0.0 + 0.0 + 0.0 + 0.0 + 0.0) = 2.1 + 1.0 = 3.1\n",
        "  mask.sum() would calculate (1 + 1 + 1 + 1 + 1 + 1 + 0 + 0) + (1 + 1 + 1 + 0 + 0 + 0 + 0 + 0) = 6 + 3 = 9 (since True is treated as 1 and False as 0)\n",
        "  average_loss would then be 3.1 / 9 ≈ 0.344\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QPo2_Vxqy4Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "batch_size, seq_len, vocab_size = 2, 5, 10\n",
        "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
        "target = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "criterion = LabelSmoothedCrossEntropy(epsilon=0.1, ignore_index=0)\n",
        "loss = criterion(logits, target)\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "eIGqd4XdD9wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model's Role:\n",
        "\n",
        "The Transformer model is designed to process sequences of data, like text. In the context of predicting the next word, it takes a sequence of words as input and outputs a probability distribution over its vocabulary for the next word.\n",
        "Essentially, for each word in the vocabulary, the model assigns a probability representing how likely it is to be the next word in the sequence.\n",
        "NLL and Cross-Entropy's Purpose:\n",
        "\n",
        "NLL (Negative Log Likelihood): NLL is a way to measure how well the model's predicted probability distribution matches the true distribution (i.e., the actual next word). Lower NLL values indicate better predictions.\n",
        "Cross-Entropy: In the case of one-hot encoded targets (where the true next word has a probability of 1 and all others have 0), cross-entropy is mathematically equivalent to NLL. It serves as the loss function during training, guiding the model to adjust its parameters and improve its predictions.\n",
        "Minimizing the Loss: The training process involves iteratively adjusting the model's parameters to minimize the NLL (or cross-entropy) loss. By minimizing this loss, the model learns to assign higher probabilities to the correct next word and lower probabilities to incorrect words.\n",
        "Label Smoothing's Refinement:\n",
        "\n",
        "Label smoothing is a technique used to prevent the model from becoming overconfident in its predictions. It slightly modifies the target distribution (making it not strictly one-hot), encouraging the model to be less certain and more robust to noisy or unexpected data.\n",
        "In this case, the loss function is called \"Label Smoothed Cross-Entropy\" because it applies label smoothing to the cross-entropy loss. It still essentially aims to minimize NLL (or a close approximation), but with a smoother target distribution.\n",
        "In Simple Terms:\n",
        "\n",
        "The Transformer model tries to guess the next word in a sequence by assigning probabilities to each word in its vocabulary.\n",
        "NLL and cross-entropy are used to measure how good the model's guesses are compared to the actual next word.\n",
        "During training, the model is adjusted to make better guesses by minimizing the NLL or cross-entropy loss.\n",
        "Label smoothing is a technique to make the model's guesses less overconfident and more adaptable.\n",
        "So, you're essentially right! The Transformer model predicts the next word, and NLL/cross-entropy helps it learn to pick the right word with higher probability by minimizing the loss function during training. Label smoothing adds a layer of refinement to the process."
      ],
      "metadata": {
        "id": "8beK7_15IeDd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-NgheWwSIfpk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}