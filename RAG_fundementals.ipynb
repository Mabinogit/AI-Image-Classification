{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOJrt0mN3RsMf/V/Poul8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mabinogit/AI-Image-Classification/blob/main/RAG_fundementals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP32QfhnZ7Ah"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import chromadb\n",
        "from openai import OpenAI\n",
        "from chromadb.utils import embedding_functions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "    api_key=openai_key,\n",
        "    model_name=\"text-embedding-ada-002\"\n",
        ")"
      ],
      "metadata": {
        "id": "Ha2m2TkseDon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIALIZE THE CHROMA CLIEN WITH PERSISTENCE\n",
        "\n",
        "chroam_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "collection_name = \"rag_collection\"\n",
        "collection = chroam_client.get_or_create_collection(name=collection_name, embedding_function=openai_ef)\n"
      ],
      "metadata": {
        "id": "2gDVo8xsitct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "yRyM-BeGjydw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to load data into list\n",
        "I want to be able to not only load text files but also pdf files this includes files that contian words and pictures"
      ],
      "metadata": {
        "id": "VNC5qQ_8k4iW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load documents from directory\n",
        "\n",
        "def load_documents(directory):\n",
        "  print(f\"Loading documents from {directory}\")\n",
        "  documents = []\n",
        "  for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".txt\"):\n",
        "      with open(os.path.join(directory, filename), \"r\",) as f:\n",
        "        content = f.read()\n",
        "        documents.append(content)\n",
        "  return documents\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lWLhEp2bj4id"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funtion to split data into chunks\n",
        "I want to be able to split all kind of data not only text into chunks"
      ],
      "metadata": {
        "id": "M-s4NjwLoGxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the more overlap the more context will be kept because the content will overlap\n",
        "def split_text(text, chunk_size=1000, chunk_overlap=20):\n",
        "  print(f\"Splitting text into chunks of {chunk_size} characters, with {chunk_overlap} characters overlap\")\n",
        "  chunks = []\n",
        "  start = 0\n",
        "  while start < len(text):\n",
        "    end = start + chunk_size\n",
        "    chunks.append(text[start:end])\n",
        "    start = end - chunk_overlap\n",
        "  print(f\"Split text into {len(chunks)} chunks\")\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "d_hsJ_XRoFHE"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from directory\n",
        "directory = \"./data\"\n",
        "documents = load_documents(directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "5faTB9qO6DCj",
        "outputId": "6be57ec6-02df-451b-8656-5cd45ec28190"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading documents from ./data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-577ca0ed75b2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load data from directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-86-d9eb36e4e7e9>\u001b[0m in \u001b[0;36mload_documents\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading documents from {directory}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split loaded documents into chunks\n",
        "\n",
        "\n",
        "chunked_documents = []\n",
        "for doc in documents:\n",
        "  chunks = split_text(doc[\"text\"])\n",
        "  print(\"=== splitting docs into chunks===\")\n",
        "  for i, chunk in enumerate(chunks):\n",
        "    chunked_documents.append({\"id\": f\"{doc['id']})-{i}\", \"text\": chunk})\n",
        "\n",
        "print(f\"split documents int {len(chunked_documents)} chuncks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "sOhUY28_0iGD",
        "outputId": "e48af90c-99e8-46d3-f867-55b9722daa39"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'documents' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-7f135e03fa72>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchunked_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== splitting docs into chunks===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to transform chunks into embeddings\n",
        "\n",
        "Be able to embedd all kinds of data to be stored into vector data base"
      ],
      "metadata": {
        "id": "TmX0emshqTnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloaded model to handel all embedding - all-MiniLM-L6-v2"
      ],
      "metadata": {
        "id": "LA_uymX-wn0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "filesnames = [ \"config.json\",\"config_sentence_transformers.json\",\"data_config.json\",\n",
        "              \"model.safetensors\",\"modules.json\", \"pytorch_model.bin\",\"rust_model.ot\",\n",
        "               \"sentence_bert_config.json\",\"special_tokens_map.json\",\"tf_model.h5\",\n",
        "               \"tokenizer.json\",\"tokenizer_config.json\",\"train_script.py\",\n",
        "               \"vocab.txt\"\n",
        "]\n",
        "\n",
        "for filename in filesnames:\n",
        "  download_path = hf_hub_download(repo_id=model_id,filename = filename, token = hugging_api)\n",
        "  print(download_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "c0XmYFVpqNne"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to generate embedding using all-MiniLM-L6-v2"
      ],
      "metadata": {
        "id": "rjFgjvKDwyUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getall_MiniLM_L6_v2_embeddings(text):\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "\n",
        "  model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "  embeddings = model.encode(text)\n",
        "  print(\"=== GENERATING EMBEDDINGS ===\")\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "2EYWcPlovmf-"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embedding for the document chuncks\n",
        "for doc in chunks:\n",
        "  print(\"=== GENERATING EMBEDDINGS ===\")\n",
        "  doc[\"embedding\"] = getall_MiniLM_L6_v2_embeddings(doc[\"text\"])\n",
        "print(\"=== EMBEDDINGS GENERATED ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "u1cnhv4Rxzkm",
        "outputId": "ef179ec5-12f6-4340-a6dc-378d1bd74f8c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chunks' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-3f662ca644eb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate embedding for the document chuncks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== GENERATING EMBEDDINGS ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetall_MiniLM_L6_v2_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== EMBEDDINGS GENERATED ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Storing data into Vector Database"
      ],
      "metadata": {
        "id": "nH7hn2BT9xyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to have a data base that has both the chunked and and its equvillent embedded data"
      ],
      "metadata": {
        "id": "9eZ_BNuY6VYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upsert documents with embedding into Chromadb\n",
        "\n",
        "for doc in chuncked_documents:\n",
        "  print(\"=== inserting chunks into db ;;; ===\")\n",
        "  collection.upsert( ids=[doc[\"id\"]],\n",
        "                    documents=[doc[\"text\"]],\n",
        "                    embeddings=[doc[\"embedding\"]])\n",
        "print(\"=== DOCUMENTS UPSERTED ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "a6Fekf-r6d4A",
        "outputId": "fc55b0fc-6f2a-474f-af3f-aff14ec926c3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chuncked_documents' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-4239ba95b1e7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# upsert documents with embedding into Chroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchuncked_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== inserting chunks into db ;;; ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   collection.upsert( ids=[doc[\"id\"]], \n",
            "\u001b[0;31mNameError\u001b[0m: name 'chuncked_documents' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Searching for data In Vector DataBase"
      ],
      "metadata": {
        "id": "v2A5Mbrq93kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to query our documents. look for document that closest to the user query\n",
        "\n",
        "def query_documents(question, n_results = 2):\n",
        "  # query_embedding = get all-Mini_embedding(question)\n",
        "  results =collection.quesry(query_texts = question, n_results = n_results)\n",
        "\n",
        "  #Extract the relevent chunks\n",
        "  relevent_chunks = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
        "  print(\"===Returninh relevent chunks ===\")\n",
        "  return relevent_chunks\n",
        ""
      ],
      "metadata": {
        "id": "6l0IJUph8HMH"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating relevent Response"
      ],
      "metadata": {
        "id": "2o8ICgmp99Ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logging in to hugging face and accessing model"
      ],
      "metadata": {
        "id": "41RoUTJk-UNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "from google.colab import userdata\n",
        "hugging_api = userdata.get('Hugging_rag_api')\n",
        "\n",
        "# login into hugging face client\n",
        "from huggingface_hub import login\n",
        "login(token=hugging_api)\n"
      ],
      "metadata": {
        "id": "IF8yimD1CHyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading model"
      ],
      "metadata": {
        "id": "R-c81By--eXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"openai-community/gpt2\"\n",
        "filesnames = [ \"config.json\" ,\"flax_model.msgpack\" ,\"merges.txt\" ,\n",
        "              \"model.safetensors\" ,\"pytorch_model.bin\" ,\"rust_model.ot\" ,\"tf_model.h5\",\n",
        "              \"tokenizer.json\" ,\"tokenizer_config.json\" ,\"vocab.json\" ,\n",
        "              \"64-fp16.tflite\" ,\"64-fp16.tflite\" ,\"64.tflite\"\n",
        "]\n",
        "\n",
        "\n",
        "# Download LLM Files\n",
        "for filename in filesnames:\n",
        "  download_path = hf_hub_download(repo_id=model_id,filename=filename, token = hugging_api)\n",
        "  print(download_path)\n",
        "\n",
        "\n",
        "# configure LLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text_gen = pipeline(\"text-generation\", model=model,device = -1, tokenizer=tokenizer, max_new_tokens=100)"
      ],
      "metadata": {
        "id": "3AezCYKrFMzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84fc4ec1-3459-47aa-ea22-d0d16a12751b"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/flax_model.msgpack\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/pytorch_model.bin\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/rust_model.ot\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tf_model.h5\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/64-fp16.tflite\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/64-fp16.tflite\n",
            "/root/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/64.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for generating response from LLM"
      ],
      "metadata": {
        "id": "6akzdMc787P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a response from LLM\n",
        "\n",
        "def generate_response(question, relevent_chuncks):\n",
        "  context = \"\\n\\n\".join(relevent_chuncks)\n",
        "  prompt = (\n",
        "      \"You are an assistant for question-answering tasks. Use the following pieces data/documents \"\n",
        "      \"retrieved context to answer the question. If you don't know the answer, just say that you don't know\"\n",
        "      \"Use three sentences maximum and keep the answer concise.\"\n",
        "      \"\\n\\nContext:\\n\" + context + \"\\n\\nQuestion:\\n\" + question\n",
        "  )\n",
        "  response = text_gen(prompt)\n",
        "  return response[0]['generated_text']\n",
        ""
      ],
      "metadata": {
        "id": "mdUjRrlw1BWC"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Front End System"
      ],
      "metadata": {
        "id": "xYvUkT-cB73E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "\n",
        "question = \"What is the capital of France?\"\n",
        "relevent_chunks = query_documents(question)\n",
        "response = generate_response(question, relevent_chunks)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "r38kublCA45P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}